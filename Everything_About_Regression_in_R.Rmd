---
title: "Everything About Regression in R"
author: "Brian Bruxvoort"
date: "2023-12-07"
output: html_document
---

# Start

First determine what your response variable will be: Numerical or Categorical
If numerical run linear or multiple regression depending on number of explanatory variables.
If categorical run a logistic regression.
Don't forget to recode any categorical explanatory variables as indicator variables like 0's and 1's.

# Load necssary packages, libraries, and data sets
## Libraries
```{r}
library(dplyr)
library(car)
library(tidyverse)
library(leaps)
library(PropCIs) # For prop test
library(mcprofile)
library(nnet)
library(MASS)
library(vcd)
#library(glmulti)
library(randomForest)
library(rpart)
library(rpart.plot)
library(Metrics)
library(readxl)
library(caret)
```

## Data
```{r}
murder<-read.csv(file="MURDERRATE.csv", header=TRUE, sep=",") # simple linear regression
hitters <- read.csv("Hitters.csv") # Multiple Linear Regression
hitters <- hitters %>% dplyr::select(-c(AtBat, Hits, HmRun, Runs, RBI, Walks, League, Division, PutOuts, Assists, Errors, NewLeague)) %>% drop_na(Salary) # Clean data
Gator<-read.csv("ALLIGATOR.csv", header=TRUE, sep=",") # Polynomial Models
frog<-read.csv("Bullfrog.csv", header=TRUE, sep=",") # Simple Logistic
ford<-read.csv("FordExplorer_RS.csv", header=TRUE, sep=",") # Multiple Logistic
chemical<-read.csv("MVPExer13_6.csv", header=TRUE, sep=",") # GLM
placekick <- read.csv("Placekick.csv") # Categorical Analysis
tomato <- read.csv("TomatoVirus.csv")
diet <- read.csv("Fiber.csv")
wheat <- read.csv("wheat.csv")
stoplight <- read.csv("Stoplight.csv")
crab <- read.table("Horseshoe.txt", header = TRUE)
```

# Simple Linear Regression
```{r}
#run basic SLR
murderSLR<-lm(murder.rate~weapons, data=murder)
summary(murderSLR)
```

```{r}
#make scatterplot
plot(murder$weapons, murder$murder.rate, pch=20)

#add regression line to scatterplot
abline(murderSLR)
```

```{r}
#confidence intervals for coefficients
confint(murderSLR, level=0.95)
```

```{r}
#Create ANOVA table
anova(murderSLR)
```

```{r}
#Create another ANOVA table
Anova(murderSLR)
```

```{r}
#Basic residual plots
plot(murderSLR, which=1, pch=20)  #residual vs fits
par(mfrow=c(1,2))                 
hist(resid(murderSLR))            #histograam of residuals
plot(murderSLR, which=2, pch=20)  #QQplot
par(mfrow=c(1,1))

qqPlot(murderSLR, pch=20)         #another QQplot
par(mfrow=c(1,1))
plot(resid(murderSLR), pch=20)
```

```{r}
#get confidence interval for mean response at weapons=0.5
newx<-data.frame(weapons=0.5)
predict.lm(murderSLR, newx, se.fit=TRUE, interval="confidence", level=0.95)

#get prediction interval for new individual at weapons=0.5
predict.lm(murderSLR, newx, se.fit=TRUE, interval="prediction", level=0.95)
```

# Multiple Regression
## Model Selection
```{r}
# Run full original model
model <- lm(Salary ~ ., data = hitters)
summary(model)
```

```{r}
# Mallow's Cp method of finding model with lowest Cp compared to Pr value.
leapout<-leaps(x=hitters[1:7], y=hitters$Salary, names=colnames(hitters)[1:7], method="Cp")
cbind(leapout$size, leapout$which, leapout$Cp)
```

```{r}
# Backward selection
step(model, data=hitters, direction="backward")
```

```{r}
# Create a null model to be able to run forward and step-wise selections
modelNull<-lm(Salary~1, data=hitters)
summary(modelNull)
```

```{r}
# Forward selection
step(modelNull, data=hitters, direction="forward", scope= ~Salary ~ Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks)
```

```{r}
# Step-wise selection
step(modelNull, data=hitters, direction="both", scope= ~Salary ~ Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks)
```

## Run new model
```{r}
# Run new model with best variables
newmodel <- lm(Salary ~ CRBI + Years + CRuns + CAtBat + CHits, data = hitters)
summary(newmodel)
```

## Point Prediction of Response
```{r}
# Predicted salary for player who has 186 career RBIs, has played for 9 years, has 192 career runs, 1876 career at bats, and 467 career hits.
#confidence interval for mean response
predict(newmodel, newdata= data.frame("CRBI" = 186, "Years" = 9, "CRuns" = 192, "CAtBat" = 1876, "CHits"= 467), level=0.95, interval="confidence")
#prediction interval for new individual
predict(newmodel, newdata= data.frame("CRBI" = 186, "Years" = 9, "CRuns" = 192, "CAtBat" = 1876, "CHits"= 467), level=0.95, interval="prediction")
```

## Check assumptions of Linear Regression
```{r}
#Residual plots
tresid=rstudent(newmodel)
par(mfrow=c(1,2))
hist(tresid, pch=20)

qqPlot(newmodel)
plot(newmodel$fitted.values, tresid, pch=20)
abline(h=0)
plot(tresid, pch=20, type="o")
abline(h=0)


avPlots(newmodel)
```

Issues with constant variation and normality of the residuals

## Fix non constant variation
```{r}
newmodel2 <- lm(log(Salary) ~ CRBI + Years + CRuns + CAtBat + CHits, data = hitters)
summary(newmodel2)
```

Always start by taking natural ln (log) of the response variable.

May need to take a log of one or multiple of the explanatory variables.

```{r}
# Recheck residual plots
tresid=rstudent(newmodel2)
par(mfrow=c(1,2))
hist(tresid, pch=20)

qqPlot(newmodel2)
plot(newmodel2$fitted.values, tresid, pch=20)
abline(h=0)
plot(tresid, pch=20, type="o")
abline(h=0)
```

Fixes our assumptions but now some explanatory variables aren't significant. Rerun variable selection section.

```{r}
# Create a null model to be able to run forward and step-wise selections
modelNull2<-lm(log(Salary)~1, data=hitters)
summary(modelNull2)
```

```{r}
# Backward selection
step(newmodel2, data=hitters, direction="backward")
```

```{r}
# Step-wise selection
step(modelNull2, data=hitters, direction="both", scope= ~log(Salary) ~ Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks)
```

## Run final model
```{r}
finalmodel <- lm(log(Salary) ~ CRuns + CAtBat + CHits, data = hitters)
summary(finalmodel)
```

```{r}
anova(finalmodel)
```

```{r}
Anova(finalmodel)
```


## Checks for Multicollinearity
```{r}
vif(finalmodel)
```

VIFs indicate an issue with multicollinearity.

```{r}
# Standardize the variables and put back into dataset
SCRuns <- hitters$CRuns-mean(hitters$CRuns)
SCAtBat <- hitters$CAtBat-mean(hitters$CAtBat)
SCHits <- hitters$CHits-mean(hitters$CHits)
hitters2 <- cbind(hitters, SCRuns, SCAtBat, SCHits)
hitters2
```

```{r}
# Run new model with standardized variabless
finalmodel2 <- lm(Salary ~ SCRuns + SCAtBat + SCHits, data = hitters2)
summary(finalmodel2)
vif(finalmodel2)
```

## Partial F-tests
```{r}
# Partial F-test of full original model and new final model
anova(finalmodel, model)
```

## Checks for Outliers and Influential Points
```{r}
#residuals for outliers
summary(tresid)
qt(0.025, df=263-6, lower.tail=FALSE)
qt(0.005, df=263-6, lower.tail=FALSE)
par(mfrow=c(1,1))
plot(tresid, pch=20, type="o")
abline(h=0)
abline(h=qt(0.025, df=263-6, lower.tail=FALSE), col="blue")
abline(h=qt(0.025, df=263-6), col="blue")
abline(h=qt(0.005, df=263-6, lower.tail=FALSE), col="red")
abline(h=qt(0.005, df=263-6), col="red")
tresid[1:263]

#leverages
plot(hatvalues(finalmodel), pch=20, type="o")
abline(h=2*6/263, col="blue")  #cutoff: 2p/n

#Influential points
#Cook's distance
plot(cooks.distance(finalmodel), pch=20, type="o")
abline(h=qf(0.50, df1=6, df2=263-6), col="blue")
abline(h=1, col="red")

#DFBetas
summary(dfbetas(finalmodel))
plot(dfbetas(finalmodel)[,1], pch=20, type="o", ylim=c(-1,2))
lines(dfbetas(finalmodel)[,2], pch=20, type="o", col="blue")
lines(dfbetas(finalmodel)[,3], pch=20, type="o", col="dark green")
lines(dfbetas(finalmodel)[,4], pch=20, type="o", col="dark red")
abline(h=2/sqrt(51))     #cutoff 2/sqrt(n)
abline(h=-2/sqrt(51))

#DFfits
summary(dffits(finalmodel))
plot(dffits(finalmodel), pch=20, type="o")
abline(h=2*sqrt(6/263), col="red")     #cutoff: 2 sqrt(p/n)
abline(h=-2*sqrt(6/263), col="red")
```

## Autocorrelation test
```{r}
#Durbin-Watson Test 
library(car)
dwt(finalmodel, alternative="positive")
```

## Residuals
```{r}
#getting residuals
model.resid<-resid(finalmodel)  #standard, e
data.frame("mean"=mean(model.resid), "var"=var(model.resid)*31/29)
plot(model.resid, pch=20)
model.stresid<-rstandard(finalmodel)  #studentized, r
data.frame("mean"=mean(model.stresid), "var"=var(model.stresid))
plot(model.stresid, pch=20, col="blue")
model.rstudent<-rstudent(finalmodel)   #deleted, t
data.frame("mean"=mean(model.rstudent), "var"=var(model.rstudent))
plot(model.rstudent, pch=20, col="red")

#residual plots
qqnorm(model.rstudent, pch=20)       #normal plot
qqline(model.rstudent, col="blue")

qqPlot(finalmodel, pch=20)
hist(model.rstudent)
# Shaprio-Wilk's and Kolmogorov-Smirnov tests for normality
# larger p-values mean can't reject assumption of normality
shapiro.test(model.rstudent)
ks.test(model.rstudent, "pnorm")

plot(hitters$CHits, model.rstudent, pch=20) #resid vs CHits
abline(h=0)
plot(hitters$CRuns, model.rstudent, pch=20) #resid vs CRuns
abline(h=0)
residualPlots(finalmodel)   #from car package
```


## Find best model including all possible interaction terms
### Using R-squared Adjusted
```{r}
predictor_combinations <- regsubsets(Salary ~ .^2, data = hitters, method = "exhaustive")

# Find the best model based on different criteria (e.g., adjusted R-squared)
best_model <- summary(predictor_combinations)$which[which.max(summary(predictor_combinations)$adjr2), ]

# Display the best model
best_model_details <- summary(predictor_combinations)
best_model_details$which[which.max(best_model_details$adjr2), ]
```

### Using BIC Values
```{r}
bic_values <- summary(predictor_combinations)$bic

# Find the model with the lowest BIC (which often coincides with the lowest AIC)
best_model_index <- which.min(bic_values)

# Extract details of the best model
best_model_details <- summary(predictor_combinations)
best_model <- coef(predictor_combinations, id = best_model_index)
best_model
```

## Polynomial Models
```{r}
#cubic model
gatorCubic<-lm(weight~length+length2+length3, data=Gator)
summary(gatorCubic)
#Note: Can have R calculate the powers, but need to use I()
#  (isolates the powers)
gatorCubic2<-lm(weight~length+I(length^2)+I(length^3), data=Gator)
summary(gatorCubic2)
#note multicollinearity
vif(gatorCubic)

#Compare models
plot(Gator$length, Gator$weight, pch=20)
abline(lm(weight~length, data=Gator), col ="blue", lwd=2)
lines(55:150, gatorCubic$coef[1]+gatorCubic$coef[2]*(55:150)+gatorCubic$coef[3]*(55:150)^2
    +gatorCubic$coef[4]*(55:150)^3, col="red", lwd=2)


#recentering to reduce multicollinearity
clength<-Gator$length-mean(Gator$length)
Gator<-cbind(Gator, clength)
Gator
gatorCubic3<-lm(weight~clength+I(clength^2)+I(clength^3), data=Gator)
summary(gatorCubic3)
vif(gatorCubic3)
# can use scale() to fully standardize (still need I())
gatorCubic4<-lm(weight~scale(length)+I(scale(length)^2)+I(scale(length)^3), data=Gator)
summary(gatorCubic4)
vif(gatorCubic4)
# can also let R standardize-- and make orthogonal; not clear how to interpret
gatorpoly3<-lm(weight~poly(length,3), data=Gator)
summary(gatorpoly3)

#Using Conditional SS to check for order
gatorquintic<-lm(weight~clength+I(clength^2)+I(clength^3)+I(clength^4)+I(clength^5), data=Gator)
summary(gatorquintic)
anova(gatorquintic)
```


# Logisitc Regression
## Simple
```{r}
#Fitting logistic regression model
froglogit<-glm(mate~size, family=binomial(link="logit"), data=frog)
summary(froglogit)
oddsratio<-exp(coefficients(froglogit)["size"])
oddsratio
#Overall test of significance
#if you don't specify test="Chisq", you won't get p-value
anova(froglogit, test="Chisq") 

```

## Multiple
```{r}
#Fitting logistic regression model
fordlogit<-glm(tire.related~vehicle.age+passengers+ford, 
               family=binomial(link="logit"), data=ford)
summary(fordlogit)

#Overall test of significance
#if you don't specify test="Chisq", you won't get p-value
anova(fordlogit, test="Chisq") 
#need to define comparison model for ANOVA chi-squared test
fordnull<-glm(tire.related~1, family=binomial(link="logit"), data=ford)
anova(fordnull, fordlogit, test="Chisq")

resD<-residuals(fordlogit, type = c("deviance"))
#can't use qqPlot from car in glm
qqnorm(resD)
plot(fordlogit, which=1)
```

### Prediction
```{r}
# Function to estimate probability using the logistic regression model
estimate_probability <- function(vehicle_age, num_passengers, is_ford) {
  # Creating a data frame with user inputs
  new_data <- data.frame(vehicle.age = vehicle_age,
                         passengers = num_passengers,
                         ford = as.numeric(is_ford))
  
  # Predicting the probability using the logistic regression model
  probability <- predict(fordlogit, newdata = new_data, type = "response")
  
  return(probability)
}

# Usage example:
# Input values (replace these with your desired values)
vehicle_age_input <- 2
passengers_input <- 4
ford_input <- FALSE  # or FALSE for non-Ford

# Estimate probability using the function
estimated_prob <- estimate_probability(vehicle_age_input, passengers_input, ford_input)
print(estimated_prob)
```

# General Linear Model
```{r}
#Fitting Poisson (identity) regression model
#chemid<-glm(Count~Time, family=poisson(link="identity"), data=chemical)
#chemid<-glm(Count~Time, family=poisson(link="identity"), data=chemical, start=c(0.5, 0.5))
#summary(chemid)
#Note: glm will throw error/warning if can't converge or converges to model outside parameter range
#For these data, linear intercept would be negative (not possible in Poisson)
OLS<-lm(Count~Time, data=chemical)
summary(OLS)
#glm identity and lm models look the same, but use different estimation approaches
#lm doesn't care that it predicts outside non-negative values
WLS<-lm(Count~Time, data=chemical, weight=1/(Time))
summary(WLS)
# var is proportional to count, but have counts=0, so using time

#fitting log link model
chemlog<-glm(Count~Time, family=poisson(link="log"), data=chemical)
summary(chemlog)
#Overall test of significance
#if you don't specify test="Chisq", you won't get p-value
anova(chemlog, test="Chisq") 

#plot the fitted curve
plot(chemical$Time, chemical$Count, pch=20)
#we use the predict function to get the predicted values
#use curve to draw the curve on the graph
curve(predict(OLS, data.frame(Time=x), type="response"), col="blue", lwd=2, add=TRUE)
curve(predict(WLS, data.frame(Time=x), type="response"), col="red", lwd=2, add=TRUE)
curve(predict(chemlog, data.frame(Time=x), type="response"), col="purple", lwd=2, add=TRUE)
```

# Random Forests
```{r}
rf.x <- hitters %>% dplyr::select(-Salary)
rf.y <- hitters$Salary
tuneRF(x=rf.x, y=rf.y, stepFactor=1.5, improve=0)
```

```{r}
fit.rf <- randomForest(x=rf.x, y=rf.y, ntree=50)
importance(fit.rf)
varImpPlot(fit.rf, n.var=20)
```


